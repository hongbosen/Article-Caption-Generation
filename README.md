# RNN Based article caption generator

Dataset link : https://docs.google.com/forms/d/e/1FAIpQLSfsNAD8UNJxi5SLGeBPEgYZdWUQfSDc4r95p8eSjMlsnqibgg/viewform?c=0&w=1

This Deep Neural network program for generating Headlines for news articles. Overall model has thre files:

1. vocabularyGen : Pre-process the data from input jsonl file and generates embedding matrix, word to index mappings and glove index mapping file. Also also save container for headlines and description for training
~~~
Input:  1. dataset file = signalmedia-1m.jsonl
		2. glove vector file = glove.6B.100d.txt
		Make sure these files are present in the before calling vocabularyGen method

Output: 1. embedding file = vocabulary-embedding.pkl
		(save embedding data for future use)
~~~
2. train : 		train the network for given dataset
~~~				
Input : takes required data from containers generated by vocabularyGen.
        Make sure to run vocabularyGen before calling train.

Output: 1. trained weights = train.hdf5
		2. history of training = history.hdf5
~~~
3. predict : 		predict the headline of the text using trained model
~~~			
Input : 1. trained model weights = train.hdf5
		2. embedding file = vocabulary-embedding.pkl
    	3. text article as string given in function parameter

Output:	1. return a string with headline
~~~
### Dependencies and additional requirements:
Python version 2.7xx
libraries to be installed before running program:
1. keras
2. theano
3. pickle
4. json
5. numpy
6. sklearn
7. h5py
8. Levenshtein

Make sure all the libraries are properly installed before running program

Model parameters:
All the model parameters are stored in class constructor function

### USAGE:

for generating object of the model class, append at the last of program with the following line
```
model = HeadlineGen()
```
for generating embedding data containers:
```
model.vocabularyGen()
```
for training model (make sure to run vocabularyGen first)
```
model.train()
```
for predicting the headline:
```
string Y = model.predict(string X)
```
Note: As these model is very big and training dataset is also very big. It is adviced to run it only on servers. Training will take a lot of time.
